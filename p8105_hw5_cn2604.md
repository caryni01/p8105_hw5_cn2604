p8105_hw5_cn2604
================
Cary Ni
2022-11-04

The code chunk below imports the data in individual spreadsheets
contained in `./data/zip_data/`. To do this, I create a dataframe that
includes the list of all files in that directory and the complete path
to each file. As a next step, I iterate over paths and import data using
the read_csv. Finally, I unnest the results.

``` r
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```

The result of the previous code chunk is not tidy and some important
variables are included as parts of others. The code chunk below tides
the data using string manipulations on the file, converting from wide to
long, and selecting relevant variables.

``` r
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data,
faceted by group.

``` r
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = files, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

<img src="p8105_hw5_cn2604_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

This plot suggests high within-subject correlation – subjects who start
above average end up above average, and those that start below average
end up below average. Subjects in the control group generally don’t
change over time, but those in the experiment group increase their
outcome in a roughly linear way.

# Problem 2

``` r
homi_df = read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = paste(city, state, sep = ", "),
    outcome = case_when(
      disposition == 'Closed without arrest' ~ 'unsolved',
      disposition == 'Open/No arrest' ~ 'unsolved',
      disposition == 'Closed by arrest' ~ 'solved', 
      TRUE ~ 'unknown'
    )
  ) %>% 
  relocate(city_state, .after = state)
```

The raw dataframe has 12 variables and 52179 observations. It records
each of homicide cases across 51 cities in the US with a label of
results, which are ‘Closed without arrest’, ‘Open/No arrest’, and
‘Closed by arrest’.

Baltimore as a sample

``` r
baltimore_summary = homi_df %>% 
  filter(city_state == 'Baltimore, MD') %>% 
  summarise(
    unsolved = sum(outcome == 'unsolved'),
    number = n()
  ) %>% 
  add_column(city_name = 'Baltimore, MD')

baltimore_test = prop.test(
  x = pull(baltimore_summary, unsolved),
  n = pull(baltimore_summary, number)
) 

baltimore_result = baltimore_test %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  mutate(
    city = pull(baltimore_summary, city_name)
  ) %>% 
  relocate(city)
```

Create the function

``` r
prop_function = function(sample_df){
  sample_summary = sample_df %>% 
  summarise(
    unsolved = sum(outcome == 'unsolved'),
    number = n()
  )
  
  sample_test = 
    prop.test(
      x = pull(sample_summary, unsolved),
      n = pull(sample_summary, number)
    ) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high)
  
  sample_test
}
```

Pass cities to the function

``` r
homi_data = 
  homi_df %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(
    city_result = map(data, prop_function)
  ) %>% 
  unnest(city_result) %>% 
  select(-data)

homi_data %>% 
  head() %>% 
  knitr::kable()
```

| city_state      |  estimate |  conf.low | conf.high |
|:----------------|----------:|----------:|----------:|
| Albuquerque, NM | 0.3862434 | 0.3372604 | 0.4375766 |
| Atlanta, GA     | 0.3833505 | 0.3528119 | 0.4148219 |
| Baltimore, MD   | 0.6455607 | 0.6275625 | 0.6631599 |
| Baton Rouge, LA | 0.4622642 | 0.4141987 | 0.5110240 |
| Birmingham, AL  | 0.4337500 | 0.3991889 | 0.4689557 |
| Boston, MA      | 0.5048860 | 0.4646219 | 0.5450881 |

Create plots

``` r
homi_data %>% 
  filter(city_state != "Tulsa, AL") %>% 
  ggplot(aes(x = reorder(city_state,-estimate), y = estimate)) + 
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(
    title = "Proportion of unsolved homicides by cities",
    x = "City",
    y = "Estimate proportion",
    caption = "Data from The Washington Post")
```

<img src="p8105_hw5_cn2604_files/figure-gfm/whole_plot-1.png" width="90%" />

# Problem 3

``` r
output_list_0 = vector("list", length = 5000)
for (i in 1:5000) {
  output_list_0[[i]] = rnorm(30, 0, 5)
}
output_test_0 = map(.x = output_list_0, ~ t.test(.x, mu=0))
output_df_0 = 
  tibble(
    true_mean = "0",
    result = map(output_test_0, broom::tidy)
  ) %>% 
  unnest(result) %>% 
  select(true_mean, estimate, p.value)
```

``` r
design_func = function(mean_input) {
  output_list = vector("list", length = 5000)
  for (i in 1:5000) {
    output_list[[i]] = rnorm(30, mean_input, 5)
  }
  output_test = map(.x = output_list, ~ t.test(.x, mu=0))
  output_df = 
  tibble(
    true_mean = as.character(mean_input),
    result = map(output_test, broom::tidy)
  ) %>% 
  unnest(result) %>% 
  select(true_mean, estimate, p.value)
  
  output_df
}
```

``` r
test_mean = c(1, 2, 3, 4, 5, 6)
result_df = map_dfr(test_mean, design_func)
```

``` r
result_summary = 
  result_df %>% 
  group_by(true_mean) %>% 
  summarise(
    ave_estimate = mean(estimate), 
    prop_reject = sum(p.value < 0.05)/n()
  ) %>% 
  mutate(true_mean = as.numeric(true_mean)) 
```

``` r
result_summary %>% 
  ggplot(aes(x=true_mean, y=prop_reject)) +
  geom_point() +
  geom_smooth() +
  labs(
    x = "True mean",
    y = "Proportion of null rejection"
    )
```

<img src="p8105_hw5_cn2604_files/figure-gfm/plot-1.png" width="90%" />

As the effect sizes increase, the statistical power will also increase.

``` r
result_summary %>% 
  ggplot(aes(x=true_mean, y=ave_estimate)) +
  geom_point() +
  geom_smooth() +
  labs(
    x = "True mean",
    y = "Average estimate mean"
    )
```

<img src="p8105_hw5_cn2604_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

``` r
result_df %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarise(
    ave_estimate = mean(estimate)
  ) %>% 
  mutate(true_mean = as.numeric(true_mean)) %>% 
  ggplot(aes(x=true_mean, y=ave_estimate)) +
  geom_point() +
  geom_smooth() +
  labs(
    x = "True mean",
    y = "Average estimate mean for null rejected"
    )
```

<img src="p8105_hw5_cn2604_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

For the sample with true mean μ = 1, 2, 3, the sample average of μ hat
across tests for which the null is rejected are higher than their true
means since the μ closer to μ=0 are excluded from the calculation for
failure of rejection. For true mean μ = 4, 5, 6, the sample average of μ
hat across tests for which the null is rejected are approximately equal
to their true means because there is few sample closer to μ=0 are
excluded from the calculation since the probability to failure of
rejection is too low for μ = 4, 5, 6.
